BeautifulSoup Setup
You use the BeautifulSoup library from bs4 to parse HTML content fetched via HTTP requests using requests.
For both Amazon and Flipkart, specific tags are targeted based on the HTML structure of each site to extract information about the products.
Functions for Product Details

get_title(), get_price(), get_rating(), get_review_count(),
and get_availability() are utility functions used to parse and extract respective data from the HTML content.
These functions are well-structured to handle missing attributes using exception handling (try-except),
which makes the code robust to changes in website structure or unavailable data.
Web Scraping Process

For each website, an HTTP request is made to fetch the webpage's content.
The HTML is parsed using BeautifulSoup, and then the product links are collected and iterated over to fetch product-specific data.
The data from each product is stored in dictionaries (d) for both Amazon and Flipkart, which are then converted into pandas DataFrames.
Data Export

The collected data is stored in CSV format using pandas.DataFrame.to_csv(). 
The code ensures that empty product titles are replaced with NaN and removed to keep the dataset clean.

Explanation for the Key Components
Dynamic URL Construction: By replacing spaces with +, we ensure that the query string is URL-friendly, allowing dynamic searches.
Robustness: The try-except blocks ensure the code doesn't crash if any HTML elements are missing (like price, rating, etc.).
Data Handling: Youâ€™ve ensured that any missing product titles are filtered out from the final CSV file, keeping your dataset clean.
Additional Improvements
Headers: The HEADERS variable is essential for making your requests appear like they are coming from a browser (to avoid being blocked by anti-bot measures). 
Make sure to use an up-to-date User-Agent.

You can find the User-Agent string for your browser using a tool like WhatIsMyBrowser.com.
Error Handling: Implement more error handling to check for failed requests or blocked access (e.g., using try-except around the requests.get() calls).

Concurrency: If the product list is large, you could use concurrent requests (e.g., using concurrent.futures or asyncio) to speed up scraping multiple product pages.
